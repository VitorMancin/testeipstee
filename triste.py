# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e5jykdV8IigWamwVURK99tzBn-lQ8XtQ
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from PIL import Image
from io import BytesIO
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Função para configurar o Selenium
def get_selenium_driver():
    options = Options()
    options.headless = True  # Executar o navegador em modo headless (sem interface gráfica)
    driver = webdriver.Chrome(options=options)
    return driver

# Função para Scraping do PubMed
def scrap_pubmed(keyword: str):
    st.text(f"Buscando no PubMed por: {keyword}")  # Log de depuração
    link = f'https://pubmed.ncbi.nlm.nih.gov/?term={keyword.lower()}'
    req = requests.get(link)
    soup = BeautifulSoup(req.text, 'html.parser')

    articles = soup.find_all('article', attrs={'class': 'full-docsum'})
    notice_name, notice_link, notice_data_ref = [], [], []

    for article in articles:
        title_tag = article.find('a', class_='docsum-title')
        if title_tag:
            code = title_tag['href']
            notice_link.append('https://pubmed.ncbi.nlm.nih.gov' + code)
            notice_name.append(title_tag.text.strip())

        citation = article.find('span', class_='docsum-journal-citation')
        notice_data_ref.append(citation.text if citation else 'N/A')

    df = pd.DataFrame({
        'Título': notice_name,
        'Link': notice_link,
        'Data de Referência': notice_data_ref
    })

    return df

# Função para Scraping do SciELO com Selenium
def scrap_scielo(keyword: str):
    st.text(f"Buscando no SciELO por: {keyword}")  # Log de depuração

    # Usando Selenium para navegar e esperar a carga da página
    driver = get_selenium_driver()
    link = f'https://search.scielo.org/?q={keyword.lower()}&lang=pt&count=15&from=0&output=site&sort=&format=summary&fb=&page=1'
    driver.get(link)

    # Espera até que os resultados sejam carregados
    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'result-item')))

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    # Buscar resultados na página
    articles = soup.find_all('div', class_='result-item')
    if not articles:
        st.warning("Nenhum artigo encontrado no SciELO. Verifique a estrutura da página ou se há restrições.")

    notice_name, notice_link, notice_data_ref = [], [], []

    for article in articles:
        title_tag = article.find('a', class_='title')
        if title_tag:
            notice_name.append(title_tag.text.strip())
            notice_link.append('https://scielo.org' + title_tag['href'])

        date_tag = article.find('span', class_='publication-date')
        notice_data_ref.append(date_tag.text.strip() if date_tag else 'N/A')

    df = pd.DataFrame({
        'Título': notice_name,
        'Link': notice_link,
        'Data de Publicação': notice_data_ref
    })

    return df

# Função para Scraping do LILACS com Selenium
def scrap_lilacs(keyword: str):
    st.text(f"Buscando no LILACS por: {keyword}")  # Log de depuração

    # Usando Selenium para navegar e esperar a carga da página
    driver = get_selenium_driver()
    link = f'https://lilacs.bvsalud.org/portal/?q={keyword.lower()}'
    driver.get(link)

    # Espera até que os resultados sejam carregados
    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'resultado')))

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    # Buscar resultados na página
    articles = soup.find_all('div', class_='resultado')
    if not articles:
        st.warning("Nenhum artigo encontrado no LILACS. Verifique a estrutura da página ou se há restrições.")

    notice_name, notice_link, notice_data_ref = [], [], []

    for article in articles:
        title_tag = article.find('a', class_='titulo')
        if title_tag:
            notice_name.append(title_tag.text.strip())
            notice_link.append('https://lilacs.bvsalud.org' + title_tag['href'])

        date_tag = article.find('span', class_='data')
        notice_data_ref.append(date_tag.text.strip() if date_tag else 'N/A')

    df = pd.DataFrame({
        'Título': notice_name,
        'Link': notice_link,
        'Data de Publicação': notice_data_ref
    })

    return df

# Configuração inicial da página
st.set_page_config(page_title='Scraping de Artigos', layout='wide')

# Exibir o logo da empresa
logo_url = "https://natcofarma.com/wp-content/webp-express/webp-images/uploads/2021/07/Logo-NatcoFarma.png.webp"
response = requests.get(logo_url)
logo = Image.open(BytesIO(response.content))
st.image(logo, caption="NatcoFarma", use_container_width=True)

# Menu de navegação
st.sidebar.header('Navegação')
page = st.sidebar.radio("Selecione uma página", ('PubMed', 'SciELO', 'LILACS'))

# Filtro de Data
start_date = st.sidebar.date_input('Data de início', datetime.today())
end_date = st.sidebar.date_input('Data de término', datetime.today())

# Verifica se a data de início é menor que a data de término
if start_date > end_date:
    st.sidebar.error('A data de início deve ser anterior à data de término.')

# Aba PubMed
if page == 'PubMed':
    st.header('Scraping da Página PubMed')

    with st.form(key='pubmed_scrapping'):
        keyword = st.text_input('Palavra-chave para PubMed')
        submit_button = st.form_submit_button(label='Buscar')

        if submit_button and keyword:
            df_pubmed = scrap_pubmed(keyword)
            if df_pubmed.empty:
                st.write("Nenhum resultado encontrado.")
            else:
                df_pubmed['Link'] = df_pubmed['Link'].apply(lambda x: f"[Acesse o artigo]({x})")
                st.write(df_pubmed.to_markdown(), unsafe_allow_html=True)
        elif submit_button:
            st.warning("Por favor, insira uma palavra-chave!")

# Aba SciELO
elif page == 'SciELO':
    st.header('Scraping da Página SciELO')

    with st.form(key='scielo_scrapping'):
        keyword = st.text_input('Palavra-chave para SciELO')
        submit_button = st.form_submit_button(label='Buscar')

        if submit_button and keyword:
            df_scielo = scrap_scielo(keyword)
            if df_scielo.empty:
                st.write("Nenhum resultado encontrado.")
            else:
                df_scielo['Link'] = df_scielo['Link'].apply(lambda x: f"[Acesse o artigo]({x})")
                st.write(df_scielo.to_markdown(), unsafe_allow_html=True)
        elif submit_button:
            st.warning("Por favor, insira uma palavra-chave!")

# Aba LILACS
elif page == 'LILACS':
    st.header('Scraping da Página LILACS')

    with st.form(key='lilacs_scrapping'):
        keyword = st.text_input('Palavra-chave para LILACS')
        submit_button = st.form_submit_button(label='Buscar')

        if submit_button and keyword:
            df_lilacs = scrap_lilacs(keyword)
            if df_lilacs.empty:
                st.write("Nenhum resultado encontrado.")
            else:
                df_lilacs['Link'] = df_lilacs['Link'].apply(lambda x: f"[Acesse o artigo]({x})")
                st.write(df_lilacs.to_markdown(), unsafe_allow_html=True)
        elif submit_button:
            st.warning("Por favor, insira uma palavra-chave!")